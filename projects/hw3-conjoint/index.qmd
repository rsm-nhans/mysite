---
title: "Multinomial Logit Model"
author: "Namaah Hans"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
```{r}
# set seed for reproducibility
set.seed(123)

# define attributes
brand <- c("N", "P", "H") # Netflix, Prime, Hulu
ad <- c("Yes", "No")
price <- seq(8, 32, by=4)

# generate all possible profiles
profiles <- expand.grid(
    brand = brand,
    ad = ad,
    price = price
)
m <- nrow(profiles)

# assign part-worth utilities (true parameters)
b_util <- c(N = 1.0, P = 0.5, H = 0)
a_util <- c(Yes = -0.8, No = 0.0)
p_util <- function(p) -0.1 * p

# number of respondents, choice tasks, and alternatives per task
n_peeps <- 100
n_tasks <- 10
n_alts <- 3

# function to simulate one respondent’s data
sim_one <- function(id) {
  
    datlist <- list()
    
    # loop over choice tasks
    for (t in 1:n_tasks) {
        
        # randomly sample 3 alts (better practice would be to use a design)
        dat <- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])
        
        # compute deterministic portion of utility
        dat$v <- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |> round(10)
        
        # add Gumbel noise (Type I extreme value)
        dat$e <- -log(-log(runif(n_alts)))
        dat$u <- dat$v + dat$e
        
        # identify chosen alternative
        dat$choice <- as.integer(dat$u == max(dat$u))
        
        # store task
        datlist[[t]] <- dat
    }
    
    # combine all tasks for one respondent
    do.call(rbind, datlist)
}

# simulate data for all respondents
conjoint_data <- do.call(rbind, lapply(1:n_peeps, sim_one))

# remove values unobservable to the researcher
conjoint_data <- conjoint_data[ , c("resp", "task", "brand", "ad", "price", "choice")]

# clean up
rm(list=setdiff(ls(), "conjoint_data"))
```
::::



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

To estimate the multinomial logit model, we first reshape the data into a flat structure with one row per alternative. We then create dummy variables for the categorical attributes:

- `netflix` and `prime` are dummy variables for the `brand` attribute (Hulu is the reference)
- `ads` is a dummy variable for whether advertisements are shown
- `price` is numeric
- `choice` is a binary indicator for whether the alternative was selected

This prepares the dataset for use in both the MLE and Bayesian estimators.

```{r}
library(dplyr)

# Convert categorical variables to dummies
data_mnl <- conjoint_data %>%
  mutate(
    netflix = ifelse(brand == "N", 1, 0),
    prime   = ifelse(brand == "P", 1, 0),
    ads     = ifelse(ad == "Yes", 1, 0)
  ) %>%
  select(resp, task, netflix, prime, ads, price, choice)

# Preview data
head(data_mnl)
```




## 4. Estimation via Maximum Likelihood

We define the log-likelihood for the multinomial logit model as:

```{r}
log_likelihood <- function(beta, X, y, n_choices = 3) {
  utilities <- X %*% beta
  utility_mat <- matrix(utilities, ncol = n_choices, byrow = TRUE)
  
  # Convert utilities to choice probabilities
  exp_util <- exp(utility_mat)
  probs <- exp_util / rowSums(exp_util)
  
  # Organize y into matrix form (for each task)
  y_mat <- matrix(y, ncol = n_choices, byrow = TRUE)
  
  # Get log probabilities only for chosen options
  log_probs <- log(probs)[y_mat == 1]
  
  # Return NEGATIVE log-likelihood (because optim minimizes)
  return(-sum(log_probs))
}
```


```{r}
# Create design matrix and response
X <- as.matrix(data_mnl[, c("netflix", "prime", "ads", "price")])
y <- data_mnl$choice
```

```{r}
# Estimate parameters using BFGS
mle_result <- optim(
  par = rep(0, 4), 
  fn = log_likelihood,
  X = X,
  y = y,
  method = "BFGS",
  hessian = TRUE
)

# Extract estimates
beta_hat <- mle_result$par

# Standard errors from Hessian
se <- sqrt(diag(solve(mle_result$hessian)))

# 95% confidence intervals
ci <- cbind(
  lower = beta_hat - 1.96 * se,
  upper = beta_hat + 1.96 * se
)

# Nicely formatted results
mle_summary <- data.frame(
  Coefficient = c("β_netflix", "β_prime", "β_ads", "β_price"),
  Estimate = round(beta_hat, 3),
  StdError = round(se, 3),
  CI_Lower = round(ci[,1], 3),
  CI_Upper = round(ci[,2], 3)
)

mle_summary
```




## 5. Estimation via Bayesian Methods

We now estimate the model using Bayesian methods via a Metropolis-Hastings MCMC algorithm. We assume:

- `β_netflix`, `β_prime`, and `β_ads` ~ N(0, 5)
- `β_price` ~ N(0, 1)

We run 11,000 steps and discard the first 1,000 as burn-in, keeping the last 10,000 samples.

```{r}
# 1. Log-prior
log_prior <- function(beta) {
  dnorm(beta[1], 0, 5, log = TRUE) + 
  dnorm(beta[2], 0, 5, log = TRUE) +
  dnorm(beta[3], 0, 5, log = TRUE) +
  dnorm(beta[4], 0, 1, log = TRUE)
}
```

```{r}
# 2. Log-posterior = log-likelihood + log-prior
log_posterior <- function(beta, X, y) {
  -log_likelihood(beta, X, y) + log_prior(beta)
}
```

```{r}
# 3. Metropolis-Hastings MCMC sampler
run_mcmc <- function(X, y, steps = 11000) {
  chain <- matrix(NA, nrow = steps, ncol = 4)
  beta <- rep(0, 4)
  chain[1, ] <- beta
  
  for (s in 2:steps) {
    proposal <- beta + c(rnorm(3, 0, sqrt(0.05)), rnorm(1, 0, sqrt(0.005)))
    
    log_accept_ratio <- log_posterior(proposal, X, y) - log_posterior(beta, X, y)
    
    if (log(runif(1)) < log_accept_ratio) {
      beta <- proposal
    }
    
    chain[s, ] <- beta
  }
  
  return(chain)
}
```

```{r}
# 4. Run MCMC and discard burn-in
set.seed(42)
samples <- run_mcmc(X, y)
post_samples <- samples[1001:11000, ]
```
```{r}
# 5. Posterior summary stats
post_mean <- colMeans(post_samples)
post_sd <- apply(post_samples, 2, sd)
post_ci <- t(apply(post_samples, 2, quantile, probs = c(0.025, 0.975)))

posterior_summary <- data.frame(
  Coefficient = c("β_netflix", "β_prime", "β_ads", "β_price"),
  Mean = round(post_mean, 3),
  SD = round(post_sd, 3),
  CI_Lower = round(post_ci[, 1], 3),
  CI_Upper = round(post_ci[, 2], 3)
)

posterior_summary
```
```{r}
# 6. Trace plot and histogram for β_price
par(mfrow = c(1, 2))  # side-by-side plots

plot(post_samples[, 4], type = "l", main = "Trace Plot: β_price", ylab = "β_price")
hist(post_samples[, 4], breaks = 40, main = "Posterior: β_price", xlab = "β_price")

par(mfrow = c(1, 1))  # reset layout
```


## 6. Discussion

The results from both the maximum likelihood and Bayesian estimation approaches tell a consistent story about consumer preferences. The positive coefficients for both $\beta_\text{Netflix}$ (MLE: 0.941, Bayes: 0.949) and $\beta_\text{Prime}$ (MLE: 0.502, Bayes: 0.496) indicate that, all else equal, consumers prefer Netflix the most, followed by Amazon Prime, with Hulu as the baseline. The coefficient for advertisements is negative (MLE: -0.732, Bayes: -0.743), suggesting that consumers dislike ad-supported plans relative to ad-free ones. Most notably, $\beta_\text{price}$ is negative and statistically significant in both models (MLE: -0.099, Bayes: -0.100), confirming that higher monthly prices decrease the probability of choosing a streaming option. The Bayesian posterior distribution for $\beta_\text{price}$ was tightly centered and showed good convergence, as seen in the trace and histogram plots.

The fact that the posterior means are nearly identical to the MLE estimates reinforces confidence in the model's stability and the sufficiency of the data. However, both methods estimate a single set of preference weights for all respondents. To simulate and estimate a more flexible model—such as a hierarchical (random-parameter) multinomial logit—we would allow each individual to have their own set of $\beta$s drawn from a common population distribution. This would involve simulating individual-level $\beta_i$ values and fitting the model using hierarchical Bayesian methods (e.g., Gibbs sampling or hierarchical MCMC).

This hierarchical approach is often more realistic in applied conjoint analysis because it captures the heterogeneity of preferences that naturally exists across consumers.









