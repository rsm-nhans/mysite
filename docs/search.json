[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nNamaah Hans\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nNamaah Hans\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nK-Means Clustering with Palmer Penguins\n\n\n\n\n\n\nNamaah Hans\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nNamaah Hans\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNamaah Hans\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hw3-conjoint/index.html",
    "href": "projects/hw3-conjoint/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3-conjoint/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3-conjoint/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3-conjoint/index.html#simulate-conjoint-data",
    "href": "projects/hw3-conjoint/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "projects/hw3-conjoint/index.html#preparing-the-data-for-estimation",
    "href": "projects/hw3-conjoint/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo estimate the multinomial logit model, we first reshape the data into a flat structure with one row per alternative. We then create dummy variables for the categorical attributes:\n\nnetflix and prime are dummy variables for the brand attribute (Hulu is the reference)\nads is a dummy variable for whether advertisements are shown\nprice is numeric\nchoice is a binary indicator for whether the alternative was selected\n\nThis prepares the dataset for use in both the MLE and Bayesian estimators.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Convert categorical variables to dummies\ndata_mnl &lt;- conjoint_data %&gt;%\n  mutate(\n    netflix = ifelse(brand == \"N\", 1, 0),\n    prime   = ifelse(brand == \"P\", 1, 0),\n    ads     = ifelse(ad == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(resp, task, netflix, prime, ads, price, choice)\n\n# Preview data\nhead(data_mnl)\n\n    resp task netflix prime ads price choice\n31     1    1       1     0   1    28      1\n15     1    1       0     0   1    16      0\n14     1    1       0     1   1    16      0\n37     1    2       1     0   1    32      0\n141    1    2       0     1   1    16      1\n25     1    2       1     0   1    24      0"
  },
  {
    "objectID": "projects/hw3-conjoint/index.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3-conjoint/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe define the log-likelihood for the multinomial logit model as:\n\nlog_likelihood &lt;- function(beta, X, y, n_choices = 3) {\n  utilities &lt;- X %*% beta\n  utility_mat &lt;- matrix(utilities, ncol = n_choices, byrow = TRUE)\n  \n  # Convert utilities to choice probabilities\n  exp_util &lt;- exp(utility_mat)\n  probs &lt;- exp_util / rowSums(exp_util)\n  \n  # Organize y into matrix form (for each task)\n  y_mat &lt;- matrix(y, ncol = n_choices, byrow = TRUE)\n  \n  # Get log probabilities only for chosen options\n  log_probs &lt;- log(probs)[y_mat == 1]\n  \n  # Return NEGATIVE log-likelihood (because optim minimizes)\n  return(-sum(log_probs))\n}\n\n\n# Create design matrix and response\nX &lt;- as.matrix(data_mnl[, c(\"netflix\", \"prime\", \"ads\", \"price\")])\ny &lt;- data_mnl$choice\n\n\n# Estimate parameters using BFGS\nmle_result &lt;- optim(\n  par = rep(0, 4), \n  fn = log_likelihood,\n  X = X,\n  y = y,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n\n# Extract estimates\nbeta_hat &lt;- mle_result$par\n\n# Standard errors from Hessian\nse &lt;- sqrt(diag(solve(mle_result$hessian)))\n\n# 95% confidence intervals\nci &lt;- cbind(\n  lower = beta_hat - 1.96 * se,\n  upper = beta_hat + 1.96 * se\n)\n\n# Nicely formatted results\nmle_summary &lt;- data.frame(\n  Coefficient = c(\"β_netflix\", \"β_prime\", \"β_ads\", \"β_price\"),\n  Estimate = round(beta_hat, 3),\n  StdError = round(se, 3),\n  CI_Lower = round(ci[,1], 3),\n  CI_Upper = round(ci[,2], 3)\n)\n\nmle_summary\n\n  Coefficient Estimate StdError CI_Lower CI_Upper\n1   β_netflix    0.941    0.111    0.724    1.159\n2     β_prime    0.502    0.111    0.284    0.719\n3       β_ads   -0.732    0.088   -0.904   -0.560\n4     β_price   -0.099    0.006   -0.112   -0.087"
  },
  {
    "objectID": "projects/hw3-conjoint/index.html#estimation-via-bayesian-methods",
    "href": "projects/hw3-conjoint/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe now estimate the model using Bayesian methods via a Metropolis-Hastings MCMC algorithm. We assume:\n\nβ_netflix, β_prime, and β_ads ~ N(0, 5)\nβ_price ~ N(0, 1)\n\nWe run 11,000 steps and discard the first 1,000 as burn-in, keeping the last 10,000 samples.\n\n# 1. Log-prior\nlog_prior &lt;- function(beta) {\n  dnorm(beta[1], 0, 5, log = TRUE) + \n  dnorm(beta[2], 0, 5, log = TRUE) +\n  dnorm(beta[3], 0, 5, log = TRUE) +\n  dnorm(beta[4], 0, 1, log = TRUE)\n}\n\n\n# 2. Log-posterior = log-likelihood + log-prior\nlog_posterior &lt;- function(beta, X, y) {\n  -log_likelihood(beta, X, y) + log_prior(beta)\n}\n\n\n# 3. Metropolis-Hastings MCMC sampler\nrun_mcmc &lt;- function(X, y, steps = 11000) {\n  chain &lt;- matrix(NA, nrow = steps, ncol = 4)\n  beta &lt;- rep(0, 4)\n  chain[1, ] &lt;- beta\n  \n  for (s in 2:steps) {\n    proposal &lt;- beta + c(rnorm(3, 0, sqrt(0.05)), rnorm(1, 0, sqrt(0.005)))\n    \n    log_accept_ratio &lt;- log_posterior(proposal, X, y) - log_posterior(beta, X, y)\n    \n    if (log(runif(1)) &lt; log_accept_ratio) {\n      beta &lt;- proposal\n    }\n    \n    chain[s, ] &lt;- beta\n  }\n  \n  return(chain)\n}\n\n\n# 4. Run MCMC and discard burn-in\nset.seed(42)\nsamples &lt;- run_mcmc(X, y)\npost_samples &lt;- samples[1001:11000, ]\n\n\n# 5. Posterior summary stats\npost_mean &lt;- colMeans(post_samples)\npost_sd &lt;- apply(post_samples, 2, sd)\npost_ci &lt;- t(apply(post_samples, 2, quantile, probs = c(0.025, 0.975)))\n\nposterior_summary &lt;- data.frame(\n  Coefficient = c(\"β_netflix\", \"β_prime\", \"β_ads\", \"β_price\"),\n  Mean = round(post_mean, 3),\n  SD = round(post_sd, 3),\n  CI_Lower = round(post_ci[, 1], 3),\n  CI_Upper = round(post_ci[, 2], 3)\n)\n\nposterior_summary\n\n  Coefficient   Mean    SD CI_Lower CI_Upper\n1   β_netflix  0.949 0.119    0.731    1.192\n2     β_prime  0.496 0.108    0.279    0.686\n3       β_ads -0.743 0.106   -0.910   -0.534\n4     β_price -0.100 0.006   -0.115   -0.087\n\n\n\n# 6. Trace plot and histogram for β_price\npar(mfrow = c(1, 2))  # side-by-side plots\n\nplot(post_samples[, 4], type = \"l\", main = \"Trace Plot: β_price\", ylab = \"β_price\")\nhist(post_samples[, 4], breaks = 40, main = \"Posterior: β_price\", xlab = \"β_price\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))  # reset layout"
  },
  {
    "objectID": "projects/hw3-conjoint/index.html#discussion",
    "href": "projects/hw3-conjoint/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nThe results from both the maximum likelihood and Bayesian estimation approaches tell a consistent story about consumer preferences. The positive coefficients for both \\(\\beta_\\text{Netflix}\\) (MLE: 0.941, Bayes: 0.949) and \\(\\beta_\\text{Prime}\\) (MLE: 0.502, Bayes: 0.496) indicate that, all else equal, consumers prefer Netflix the most, followed by Amazon Prime, with Hulu as the baseline. The coefficient for advertisements is negative (MLE: -0.732, Bayes: -0.743), suggesting that consumers dislike ad-supported plans relative to ad-free ones. Most notably, \\(\\beta_\\text{price}\\) is negative and statistically significant in both models (MLE: -0.099, Bayes: -0.100), confirming that higher monthly prices decrease the probability of choosing a streaming option. The Bayesian posterior distribution for \\(\\beta_\\text{price}\\) was tightly centered and showed good convergence, as seen in the trace and histogram plots.\nThe fact that the posterior means are nearly identical to the MLE estimates reinforces confidence in the model’s stability and the sufficiency of the data. However, both methods estimate a single set of preference weights for all respondents. To simulate and estimate a more flexible model—such as a hierarchical (random-parameter) multinomial logit—we would allow each individual to have their own set of \\(\\beta\\)s drawn from a common population distribution. This would involve simulating individual-level \\(\\beta_i\\) values and fitting the model using hierarchical Bayesian methods (e.g., Gibbs sampling or hierarchical MCMC).\nThis hierarchical approach is often more realistic in applied conjoint analysis because it captures the heterogeneity of preferences that naturally exists across consumers."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) using the seaborn and pandas libraries.\n\n\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n# Load dataset\nmpg = sns.load_dataset(\"mpg\").dropna()\n\n# Compute correlation\ncorr, _ = pearsonr(mpg[\"mpg\"], mpg[\"displacement\"])\nprint(f\"Correlation: {corr:.2f}\")\n\n# Plot\nsns.scatterplot(data=mpg, x=\"mpg\", y=\"displacement\", color=\"dodgerblue\")\nplt.title(\"MPG vs Displacement\")\nplt.xlabel(\"Miles per Gallon\")\nplt.ylabel(\"Engine Displacement\")\nplt.tight_layout()\nplt.show()\n\n\nCorrelation: -0.81"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n# Load dataset\nmpg = sns.load_dataset(\"mpg\").dropna()\n\n# Compute correlation\ncorr, _ = pearsonr(mpg[\"mpg\"], mpg[\"displacement\"])\nprint(f\"Correlation: {corr:.2f}\")\n\n# Plot\nsns.scatterplot(data=mpg, x=\"mpg\", y=\"displacement\", color=\"dodgerblue\")\nplt.title(\"MPG vs Displacement\")\nplt.xlabel(\"Miles per Gallon\")\nplt.ylabel(\"Engine Displacement\")\nplt.tight_layout()\nplt.show()\n\n\nCorrelation: -0.81"
  },
  {
    "objectID": "projects/hw-4/hw4_questions.html",
    "href": "projects/hw-4/hw4_questions.html",
    "title": "K-Means Clustering with Palmer Penguins",
    "section": "",
    "text": "We begin by loading the Palmer Penguins dataset and selecting only two numeric features: bill_length_mm and flipper_length_mm. These will be the input variables for clustering.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(cluster)\n\n# Load and clean data\npenguins &lt;- read_csv(\"palmer_penguins.csv\") %&gt;%\n  drop_na(bill_length_mm, flipper_length_mm) %&gt;%\n  select(bill_length_mm, flipper_length_mm)\n\nRows: 333 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmy_kmeans &lt;- function(data, k, max_iter = 100){\n  set.seed(123)\n  n &lt;- nrow(data)\n  centers &lt;- data[sample(1:n, k), ]\n  cluster_assignments &lt;- rep(0, n)\n\n  for (i in 1:max_iter){\n    # Assign clusters\n    dists &lt;- as.matrix(dist(rbind(centers, data)))[1:k, (k+1):(k+n)]\n    cluster_assignments &lt;- apply(dists, 2, which.min)\n\n    # Recompute centers\n    new_centers &lt;- sapply(1:k, function(j){\n      colMeans(data[cluster_assignments == j, ])\n    })\n    new_centers &lt;- t(new_centers)\n\n    if (all(centers == new_centers)) break\n    centers &lt;- new_centers\n  }\n\n  return(list(clusters = cluster_assignments, centers = centers))\n}\n\nresult &lt;- my_kmeans(penguins, k = 3)\npenguins$cluster_custom &lt;- as.factor(result$clusters)\n\nggplot(penguins, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_custom)) +\n  geom_point(size = 2) +\n  labs(title = \"Custom K-Means Clustering (k = 3)\")\n\n\n\n\n\n\n\n\nThe plot shows the clusters formed by my custom K-Means algorithm (k = 3). The separation looks reasonable, confirming that the implementation works as expected.\n\nk_builtin &lt;- kmeans(penguins, centers = 3)\npenguins$cluster_builtin &lt;- as.factor(k_builtin$cluster)\n\nggplot(penguins, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +\n  geom_point(size = 2) +\n  labs(title = \"Built-in KMeans Clustering (k = 3)\")\n\n\n\n\n\n\n\n\nThe built-in KMeans function (k = 3) produces similar clusters, validating the results from the custom implementation.\n\nlibrary(cluster)\n\nwcss &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\nfor (k in 2:7) {\n  set.seed(1)\n  km_out &lt;- kmeans(penguins, centers = k, nstart = 10)\n  wcss[k] &lt;- km_out$tot.withinss\n\n  sil &lt;- silhouette(km_out$cluster, dist(penguins))\n  silhouette_scores[k] &lt;- mean(sil[, 3])\n}\n\n\n# Elbow Plot\nplot(2:7, wcss[2:7], type = \"b\", pch = 19,\n     xlab = \"Number of Clusters (k)\",\n     ylab = \"Within-Cluster Sum of Squares\",\n     main = \"Elbow Method\")\n\n\n\n\n\n\n\n\nThe elbow plot suggests that 3 clusters is a good choice, as the rate of improvement in WCSS slows significantly after k = 3.\n\n# Silhouette Plot\nplot(2:7, silhouette_scores[2:7], type = \"b\", pch = 19,\n     xlab = \"Number of Clusters (k)\",\n     ylab = \"Silhouette Score\",\n     main = \"Silhouette Analysis\")\n\n\n\n\n\n\n\n\nThe silhouette scores are highest at k = 2, but k = 3 still provides a reasonable balance with better separation than higher k values."
  },
  {
    "objectID": "projects/hw-4/hw4_questions.html#a.-k-means",
    "href": "projects/hw-4/hw4_questions.html#a.-k-means",
    "title": "K-Means Clustering with Palmer Penguins",
    "section": "",
    "text": "We begin by loading the Palmer Penguins dataset and selecting only two numeric features: bill_length_mm and flipper_length_mm. These will be the input variables for clustering.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(cluster)\n\n# Load and clean data\npenguins &lt;- read_csv(\"palmer_penguins.csv\") %&gt;%\n  drop_na(bill_length_mm, flipper_length_mm) %&gt;%\n  select(bill_length_mm, flipper_length_mm)\n\nRows: 333 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmy_kmeans &lt;- function(data, k, max_iter = 100){\n  set.seed(123)\n  n &lt;- nrow(data)\n  centers &lt;- data[sample(1:n, k), ]\n  cluster_assignments &lt;- rep(0, n)\n\n  for (i in 1:max_iter){\n    # Assign clusters\n    dists &lt;- as.matrix(dist(rbind(centers, data)))[1:k, (k+1):(k+n)]\n    cluster_assignments &lt;- apply(dists, 2, which.min)\n\n    # Recompute centers\n    new_centers &lt;- sapply(1:k, function(j){\n      colMeans(data[cluster_assignments == j, ])\n    })\n    new_centers &lt;- t(new_centers)\n\n    if (all(centers == new_centers)) break\n    centers &lt;- new_centers\n  }\n\n  return(list(clusters = cluster_assignments, centers = centers))\n}\n\nresult &lt;- my_kmeans(penguins, k = 3)\npenguins$cluster_custom &lt;- as.factor(result$clusters)\n\nggplot(penguins, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_custom)) +\n  geom_point(size = 2) +\n  labs(title = \"Custom K-Means Clustering (k = 3)\")\n\n\n\n\n\n\n\n\nThe plot shows the clusters formed by my custom K-Means algorithm (k = 3). The separation looks reasonable, confirming that the implementation works as expected.\n\nk_builtin &lt;- kmeans(penguins, centers = 3)\npenguins$cluster_builtin &lt;- as.factor(k_builtin$cluster)\n\nggplot(penguins, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster_builtin)) +\n  geom_point(size = 2) +\n  labs(title = \"Built-in KMeans Clustering (k = 3)\")\n\n\n\n\n\n\n\n\nThe built-in KMeans function (k = 3) produces similar clusters, validating the results from the custom implementation.\n\nlibrary(cluster)\n\nwcss &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\nfor (k in 2:7) {\n  set.seed(1)\n  km_out &lt;- kmeans(penguins, centers = k, nstart = 10)\n  wcss[k] &lt;- km_out$tot.withinss\n\n  sil &lt;- silhouette(km_out$cluster, dist(penguins))\n  silhouette_scores[k] &lt;- mean(sil[, 3])\n}\n\n\n# Elbow Plot\nplot(2:7, wcss[2:7], type = \"b\", pch = 19,\n     xlab = \"Number of Clusters (k)\",\n     ylab = \"Within-Cluster Sum of Squares\",\n     main = \"Elbow Method\")\n\n\n\n\n\n\n\n\nThe elbow plot suggests that 3 clusters is a good choice, as the rate of improvement in WCSS slows significantly after k = 3.\n\n# Silhouette Plot\nplot(2:7, silhouette_scores[2:7], type = \"b\", pch = 19,\n     xlab = \"Number of Clusters (k)\",\n     ylab = \"Silhouette Score\",\n     main = \"Silhouette Analysis\")\n\n\n\n\n\n\n\n\nThe silhouette scores are highest at k = 2, but k = 3 still provides a reasonable balance with better separation than higher k values."
  },
  {
    "objectID": "projects/hw-4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "projects/hw-4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "K-Means Clustering with Palmer Penguins",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n### Plot the training data\n\nWe visualize the synthetic data with x1 on the x-axis, x2 on the y-axis, and color-coded by class y.\n\nlibrary(ggplot2)\n\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2) +\n  labs(title = \"Training Data with Wiggly Boundary\",\n       x = \"x1\", y = \"x2\")\n\n\n\n\n\n\n\n\nThe plot shows two classes separated by a non-linear (wiggly) boundary, making it ideal for testing KNN performance.\n\nset.seed(99)\nx1_test &lt;- runif(n, -3, 3)\nx2_test &lt;- runif(n, -3, 3)\nboundary_test &lt;- sin(4*x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ntest_dat &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n\n\nlibrary(class)\n\nk_values &lt;- 1:30\naccuracy &lt;- numeric(length(k_values))\n\nfor (k in k_values) {\n  pred &lt;- knn(train = dat[, 1:2],\n              test = test_dat[, 1:2],\n              cl = dat$y,\n              k = k)\n  accuracy[k] &lt;- mean(pred == test_dat$y)\n}\n\n\nplot(k_values, accuracy, type = \"b\", pch = 19,\n     xlab = \"k (Number of Neighbors)\",\n     ylab = \"Test Set Accuracy\",\n     main = \"KNN Accuracy for k = 1 to 30\")\n\n\n\n\n\n\n\n\nTest accuracy fluctuates across values of k, with the best performance observed around k = 29–30."
  },
  {
    "objectID": "projects/hw1-abtest/index.html",
    "href": "projects/hw1-abtest/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this experiment, the researchers randomly assigned potential donors to receive one of several different fundraising letters: a standard solicitation, a letter offering a matching grant (e.g., your donation will be doubled), or a challenge grant (e.g., we must raise $X total to unlock a match). This randomized design allows the authors to causally estimate the effect of matched donations on both the likelihood of giving and the amount given. The scale and design of the study make it a valuable real-world test of behavioral economic theories around charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1-abtest/index.html#introduction",
    "href": "projects/hw1-abtest/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this experiment, the researchers randomly assigned potential donors to receive one of several different fundraising letters: a standard solicitation, a letter offering a matching grant (e.g., your donation will be doubled), or a challenge grant (e.g., we must raise $X total to unlock a match). This randomized design allows the authors to causally estimate the effect of matched donations on both the likelihood of giving and the amount given. The scale and design of the study make it a valuable real-world test of behavioral economic theories around charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1-abtest/index.html#data",
    "href": "projects/hw1-abtest/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Compare 'mrm2' (months since last donation) by treatment group\ntreat = df[df[\"treatment\"] == 1][\"mrm2\"]\ncontrol = df[df[\"treatment\"] == 0][\"mrm2\"]\n\n# T-test\nt_stat, p_val = ttest_ind(treat, control)\nprint(f\"T-statistic: {t_stat:.2f}, P-value: {p_val:.4f}\")\n\n# Regression\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nmodel.summary()\n\nT-statistic: nan, P-value: nan\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmrm2\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.01428\n\n\nDate:\nWed, 11 Jun 2025\nProb (F-statistic):\n0.905\n\n\nTime:\n16:15:50\nLog-Likelihood:\n-1.9585e+05\n\n\nNo. Observations:\n50082\nAIC:\n3.917e+05\n\n\nDf Residuals:\n50080\nBIC:\n3.917e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.9981\n0.094\n138.979\n0.000\n12.815\n13.181\n\n\ntreatment\n0.0137\n0.115\n0.119\n0.905\n-0.211\n0.238\n\n\n\n\n\n\n\n\nOmnibus:\n8031.352\nDurbin-Watson:\n2.004\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12471.135\n\n\nSkew:\n1.163\nProb(JB):\n0.00\n\n\nKurtosis:\n3.751\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe variable mrm2 (months since last donation) is used as a balance test to assess whether the randomization produced statistically similar treatment and control groups. A t-test comparing the two groups produces a p-value of approximately [your p-value here], and the linear regression confirms this with a treatment coefficient that is [close to zero / not significant].\nThis suggests that the assignment to treatment was successful with respect to this variable — neither group had donors who were systematically more or less recent. This kind of check is important because it supports the internal validity of the experiment. That’s why Table 1 is included in the paper: to demonstrate that the random assignment worked and that any outcome differences later on can be interpreted causally."
  },
  {
    "objectID": "projects/hw1-abtest/index.html#experimental-results",
    "href": "projects/hw1-abtest/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a bar plot showing mean of `gave` (which is a binary variable)\nsns.barplot(data=df, x=\"treatment\", y=\"gave\", ci=None)\nplt.title(\"Proportion Who Donated by Treatment Group\")\nplt.xlabel(\"Treatment Group (0 = Control, 1 = Treatment)\")\nplt.ylabel(\"Proportion Donated\")\nplt.ylim(0, 0.05)\nplt.show()\n\n/var/folders/mk/v34kq_f92d3dnrbv9pvpx1q80000gn/T/ipykernel_60055/702351802.py:5: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.barplot(data=df, x=\"treatment\", y=\"gave\", ci=None)\n\n\n\n\n\n\n\n\n\nThe barplot shows that a higher proportion of people donated when assigned to the treatment group compared to the control group.\n\n# T-test on donation behavior\ngave_treat = df[df[\"treatment\"] == 1][\"gave\"]\ngave_control = df[df[\"treatment\"] == 0][\"gave\"]\n\nfrom scipy.stats import ttest_ind\nt_stat, p_val = ttest_ind(gave_treat, gave_control)\nprint(f\"T-statistic: {t_stat:.2f}, P-value: {p_val:.4f}\")\n\n# Linear regression: gave ~ treatment\nimport statsmodels.formula.api as smf\nmodel_gave = smf.ols(\"gave ~ treatment\", data=df).fit()\nmodel_gave.summary()\n\nT-statistic: 3.10, P-value: 0.0019\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n9.618\n\n\nDate:\nWed, 11 Jun 2025\nProb (F-statistic):\n0.00193\n\n\nTime:\n16:15:50\nLog-Likelihood:\n26630.\n\n\nNo. Observations:\n50083\nAIC:\n-5.326e+04\n\n\nDf Residuals:\n50081\nBIC:\n-5.324e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n0.016\n0.020\n\n\ntreatment\n0.0042\n0.001\n3.101\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\n\nOmnibus:\n59814.280\nDurbin-Watson:\n2.005\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4317152.727\n\n\nSkew:\n6.740\nProb(JB):\n0.00\n\n\nKurtosis:\n46.440\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression shows that being assigned to the treatment group increased the probability of donating by about 0.42 percentage points, from 1.79% to roughly 2.21%. This effect is statistically significant (p = 0.0019), suggesting that matched donation offers do modestly increase charitable giving.\n\nimport statsmodels.api as sm\n\n# Probit model\ndf[\"intercept\"] = 1\nprobit_model = sm.Probit(df[\"gave\"], df[[\"intercept\", \"treatment\"]])\nprobit_results = probit_model.fit()\nprobit_results.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 11 Jun 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n16:15:51\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nintercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\nThe probit regression shows that being assigned to the treatment group significantly increases the likelihood of donating (p = 0.002). While the coefficient isn’t directly interpretable, the positive and significant effect confirms the treatment’s impact on donation behavior, consistent with results in Table 3 of the original paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Separate groups by match ratio\nratio1 = df[(df[\"ratio2\"] == 0) & (df[\"ratio3\"] == 0)]\nratio2 = df[df[\"ratio2\"] == 1]\nratio3 = df[df[\"ratio3\"] == 1]\n\n# T-tests\nfrom scipy.stats import ttest_ind\n\nprint(\"1:1 vs 2:1\")\nt12 = ttest_ind(ratio1[\"gave\"], ratio2[\"gave\"])\nprint(f\"T-statistic: {t12.statistic:.2f}, P-value: {t12.pvalue:.4f}\")\n\nprint(\"2:1 vs 3:1\")\nt23 = ttest_ind(ratio2[\"gave\"], ratio3[\"gave\"])\nprint(f\"T-statistic: {t23.statistic:.2f}, P-value: {t23.pvalue:.4f}\")\n\n1:1 vs 2:1\nT-statistic: -2.30, P-value: 0.0213\n2:1 vs 3:1\nT-statistic: -0.05, P-value: 0.9600\n\n\nThe t-test shows that the 2:1 match rate significantly increased donations compared to the 1:1 match rate (p = 0.0213), suggesting that a higher match can boost response rates. However, there is no significant difference between 2:1 and 3:1 (p = 0.96), indicating that increasing the match beyond 2:1 does not further increase giving. This supports the idea of diminishing returns for very high match ratios.\n\n# OLS regression with match ratio dummies (baseline = 1:1)\nimport statsmodels.formula.api as smf\nmodel_match = smf.ols(\"gave ~ ratio2 + ratio3\", data=df).fit()\nmodel_match.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n4.117\n\n\nDate:\nWed, 11 Jun 2025\nProb (F-statistic):\n0.0163\n\n\nTime:\n16:15:51\nLog-Likelihood:\n26629.\n\n\nNo. Observations:\n50083\nAIC:\n-5.325e+04\n\n\nDf Residuals:\n50080\nBIC:\n-5.323e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0190\n0.001\n22.306\n0.000\n0.017\n0.021\n\n\nratio2\n0.0036\n0.002\n2.269\n0.023\n0.000\n0.007\n\n\nratio3\n0.0037\n0.002\n2.332\n0.020\n0.001\n0.007\n\n\n\n\n\n\n\n\nOmnibus:\n59815.856\nDurbin-Watson:\n2.005\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4317637.927\n\n\nSkew:\n6.741\nProb(JB):\n0.00\n\n\nKurtosis:\n46.443\nCond. No.\n3.16\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe OLS regression shows that both the 2:1 and 3:1 match ratios significantly increased the probability of donating compared to the 1:1 match (p = 0.023 and p = 0.020, respectively). The estimated increases in donation likelihood were around 0.36–0.37 percentage points. While both higher match rates are effective, the similarity in their effects suggests that raising the match beyond 2:1 may not yield substantially greater impact, consistent with the idea of diminishing returns.\n\n# Response rate differences\nrate_1to1 = ratio1[\"gave\"].mean()\nrate_2to1 = ratio2[\"gave\"].mean()\nrate_3to1 = ratio3[\"gave\"].mean()\n\nprint(f\"1:1: {rate_1to1:.4f}, 2:1: {rate_2to1:.4f}, 3:1: {rate_3to1:.4f}\")\nprint(f\"2:1 - 1:1 = {rate_2to1 - rate_1to1:.4f}\")\nprint(f\"3:1 - 2:1 = {rate_3to1 - rate_2to1:.4f}\")\n\n1:1: 0.0190, 2:1: 0.0226, 3:1: 0.0227\n2:1 - 1:1 = 0.0036\n3:1 - 2:1 = 0.0001\n\n\nMoving from a 1:1 to a 2:1 match rate increased the donation rate by 0.36 percentage points, while increasing from 2:1 to 3:1 added only 0.01 percentage points. This suggests that while higher match ratios do increase giving, the marginal benefit flattens out, supporting the idea of diminishing returns for very large match offers.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# OLS regression: amount donated ~ treatment (all participants)\nimport statsmodels.formula.api as smf\n\nmodel_amt = smf.ols(\"amount ~ treatment\", data=df).fit()\nmodel_amt.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.461\n\n\nDate:\nWed, 11 Jun 2025\nProb (F-statistic):\n0.0628\n\n\nTime:\n16:15:51\nLog-Likelihood:\n-1.7946e+05\n\n\nNo. Observations:\n50083\nAIC:\n3.589e+05\n\n\nDf Residuals:\n50081\nBIC:\n3.589e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.8133\n0.067\n12.063\n0.000\n0.681\n0.945\n\n\ntreatment\n0.1536\n0.083\n1.861\n0.063\n-0.008\n0.315\n\n\n\n\n\n\n\n\nOmnibus:\n96861.113\nDurbin-Watson:\n2.008\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n240735713.635\n\n\nSkew:\n15.297\nProb(JB):\n0.00\n\n\nKurtosis:\n341.269\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression shows that the treatment group donated $0.15 more on average than the control group, but this difference is not statistically significant (p = 0.063). This suggests that while matched donations may slightly increase average giving, the effect is weak and possibly due to chance when considering all individuals, most of whom donated nothing.\n\n# Limit to only people who donated something\ndf_gave = df[df[\"gave\"] == 1]\n\n# OLS regression: amount ~ treatment (only among donors)\nmodel_amt_gave = smf.ols(\"amount ~ treatment\", data=df_gave).fit()\nmodel_amt_gave.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3374\n\n\nDate:\nWed, 11 Jun 2025\nProb (F-statistic):\n0.561\n\n\nTime:\n16:15:51\nLog-Likelihood:\n-5326.8\n\n\nNo. Observations:\n1034\nAIC:\n1.066e+04\n\n\nDf Residuals:\n1032\nBIC:\n1.067e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n40.785\n50.296\n\n\ntreatment\n-1.6684\n2.872\n-0.581\n0.561\n-7.305\n3.968\n\n\n\n\n\n\n\n\nOmnibus:\n587.258\nDurbin-Watson:\n2.031\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5623.279\n\n\nSkew:\n2.464\nProb(JB):\n0.00\n\n\nKurtosis:\n13.307\nCond. No.\n3.49\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTAmong individuals who donated, the treatment group gave $1.67 less on average than the control group, but this difference is not statistically significant (p = 0.561). This suggests that the matching donation offer did not influence how much people gave, once they decided to donate. The treatment’s impact appears to be on the likelihood of giving, not the amount given.\n\n# Plot histograms of donation amounts for treatment vs control (among those who gave)\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,5))\n\n# Treatment group\nplt.subplot(1, 2, 1)\ndf_gave[df_gave[\"treatment\"] == 1][\"amount\"].hist(bins=30, color=\"skyblue\")\nplt.axvline(df_gave[df_gave[\"treatment\"] == 1][\"amount\"].mean(), color=\"red\", linestyle=\"dashed\")\nplt.title(\"Treatment Group Donations\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Frequency\")\n\n# Control group\nplt.subplot(1, 2, 2)\ndf_gave[df_gave[\"treatment\"] == 0][\"amount\"].hist(bins=30, color=\"lightgreen\")\nplt.axvline(df_gave[df_gave[\"treatment\"] == 0][\"amount\"].mean(), color=\"red\", linestyle=\"dashed\")\nplt.title(\"Control Group Donations\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms show that the distribution of donation amounts is highly skewed in both groups, with most donations clustered at lower amounts. The red dashed lines, representing group averages, are visually similar, confirming that the treatment did not meaningfully change the amount given among donors. This reinforces earlier regression results suggesting the treatment influenced whether people gave, not how much they gave."
  },
  {
    "objectID": "projects/hw1-abtest/index.html#simulation-experiment",
    "href": "projects/hw1-abtest/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate: 10,000 draws from each distribution (treatment and control)\nn_sim = 10000\ncontrol = np.random.binomial(1, 0.018, n_sim)\ntreatment = np.random.binomial(1, 0.022, n_sim)\n\n# Compute difference in each pair of draws\ndiffs = treatment - control\n\n# Cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_sim + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 4))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\")\nplt.axhline(y=0.004, color=\"red\", linestyle=\"dashed\", label=\"True Difference (0.004)\")\nplt.xlabel(\"Simulation #\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Law of Large Numbers Demonstration\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot shows that as the number of simulations increases, the cumulative average difference between treatment and control groups stabilizes around the true difference of 0.004. This illustrates the Law of Large Numbers — with a large enough sample size, the sample average converges to the expected population value, confirming the reliability of randomized experimental results over time.\n\n\nCentral Limit Theorem\n\n# Simulate CLT for different sample sizes\nsample_sizes = [50, 200, 500, 1000]\nplt.figure(figsize=(12, 8))\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, 0.018, n)\n        treatment_sample = np.random.binomial(1, 0.022, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n\n    plt.subplot(2, 2, i + 1)\n    plt.hist(mean_diffs, bins=30, color=\"lightblue\", edgecolor=\"black\")\n    plt.axvline(0, color=\"red\", linestyle=\"dashed\")\n    plt.title(f\"Sample size = {n}\")\n    plt.xlabel(\"Difference in Means\")\n    plt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show that as the sample size increases, the distribution of simulated mean differences becomes more symmetric and concentrated around the true mean difference (≈0.004). At small sample sizes (e.g., 50), the distribution is noisy and spread out, but by size 1000, it’s tightly centered and nearly normal. This demonstrates the Central Limit Theorem, showing that the sampling distribution of the mean approaches normality as sample size grows."
  },
  {
    "objectID": "projects/hw2-mle/index.html",
    "href": "projects/hw2-mle/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Blueprinty data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Histogram of number of patents by customer status\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", kde=True, bins=20)\nplt.title(\"Number of Patents by Customer Status\")\nplt.show()\n\n# Compare mean number of patents\nprint(df.groupby(\"iscustomer\")[\"patents\"].mean())\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nThe histogram shows that both customers and non-customers of Blueprinty have similar distributions centered around 2 to 4 patents. However, customers tend to have a slightly wider spread and a longer right tail, indicating that some of them have more patents than non-customers. This suggests that firms using Blueprinty’s software may be more likely to have higher numbers of patents, supporting the marketing team’s claim.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Count of regions by customer status\nprint(df.groupby(\"iscustomer\")[\"region\"].value_counts())\n\n# Compare average age by customer status\nprint(df.groupby(\"iscustomer\")[\"age\"].mean())\n\niscustomer  region   \n0           Northeast    273\n            Southwest    245\n            Midwest      187\n            Northwest    158\n            South        156\n1           Northeast    328\n            Southwest     52\n            Midwest       37\n            South         35\n            Northwest     29\nName: count, dtype: int64\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n# Histogram for age\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", bins=20, kde=True)\nplt.title(\"Firm Age by Customer Status\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Bar chart for region distribution\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Region by Customer Status\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the output, we can see that Blueprinty customers are not evenly distributed across regions. A large majority of customers come from the Northeast, while regions like the Southwest, Midwest, South, and Northwest have far fewer customers compared to non-customers. This suggests a strong regional skew in customer distribution. Additionally, the average age of customer firms (26.9 years) is slightly higher than that of non-customers (26.1 years), indicating that Blueprinty customers may be somewhat more established. These differences suggest that region and age could be confounding factors and should be accounted for in the analysis.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for ( Y () ) is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nWe now define the log-likelihood function for the Poisson model, which takes a scalar () and a vector of observed counts (Y):\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Log-likelihood for Poisson distribution.\n    Parameters:\n      lmbda: scalar lambda value\n      y: array or list of observed counts\n    Returns:\n      total log-likelihood as a float\n    \"\"\"\n\n\n    y = np.array(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\nimport matplotlib.pyplot as plt\n\n# Observed patent counts\ny_obs = df[\"patents\"].values\n\n# Range of lambda values\nlambda_vals = np.linspace(0.1, 20, 200)\nloglikelihoods = [poisson_log_likelihood(lmbda, y_obs) for lmbda in lambda_vals]\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglikelihoods)\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood of Poisson Model Across Lambda Values\")\nplt.grid(True)\nplt.show()\n\nlambda_mle_analytical = np.mean(y_obs)\nlambda_mle_analytical\n\nfrom scipy.optimize import minimize\n\n# We minimize the negative log-likelihood\nneg_log_likelihood = lambda lmbda: -poisson_log_likelihood(lmbda[0], y_obs)\n\n# Start from an initial guess, e.g., lambda = 1\nresult = minimize(neg_log_likelihood, x0=[1], bounds=[(0.01, None)])\nlambda_mle_numerical = result.x[0]\nlambda_mle_numerical\n\nprint(f\"Analytical MLE (mean of Y): {lambda_mle_analytical:.4f}\")\nprint(f\"Numerical MLE (via optimization): {lambda_mle_numerical:.4f}\")\n\n\n\n\n\n\n\n\nAnalytical MLE (mean of Y): 3.6847\nNumerical MLE (via optimization): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now define the Poisson regression log-likelihood function, where the expected number of patents (_i) is modeled as a function of firm characteristics:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_log_likelihood(beta, y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    # Compute linear predictor\n    linear_predictor = X @ beta\n\n    # Prevent overflow in exp\n    linear_predictor = np.clip(linear_predictor, -300, 50)\n\n    lambda_i = np.exp(linear_predictor)\n\n    return np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n\nWe now estimate the Poisson regression model via maximum likelihood and compare it to the built-in GLM function.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport statsmodels.api as sm\n\n# 1. Feature engineering\ndf[\"age_sq\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# 2. Build design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_sq\"]],\n    region_dummies,\n    df[[\"iscustomer\"]]\n], axis=1)\n\ny = df[\"patents\"]\n\n# Convert to NumPy arrays with explicit dtype\nX_np = X.to_numpy(dtype=np.float64)\ny_np = y.to_numpy(dtype=np.float64)\n\n# 3. Define Poisson regression log-likelihood\ndef poisson_regression_log_likelihood(beta, y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    # Compute λ = exp(Xβ)\n    linear_predictor = X @ beta\n    lambda_i = np.exp(linear_predictor)\n\n    return np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n\n# 4. Minimize the negative log-likelihood\ndef neg_log_likelihood(beta):\n    return -poisson_regression_log_likelihood(beta, y_np, X_np)\n\ninit_beta = np.zeros(X_np.shape[1])\n\nresult = minimize(neg_log_likelihood, init_beta, method=\"BFGS\")\n\n# 5. Extract coefficients and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# 6. Output results as a table\nresults_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errors\n})\nprint(results_df)\n\n     Variable  Coefficient  Std. Error\n0   intercept     1.480059         1.0\n1         age    38.016417         1.0\n2      age_sq  1033.539585         1.0\n3   Northeast     0.640979         1.0\n4   Northwest     0.164288         1.0\n5       South     0.181562         1.0\n6   Southwest     0.295497         1.0\n7  iscustomer     0.553874         1.0\n\n\nWe now check the results using Python’s built-in statsmodels.GLM function:\n\nimport statsmodels.api as sm\n\n# Ensure all inputs are numeric\nX_clean = X.astype(float)\ny_clean = y.astype(float)\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y_clean, X_clean, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Show summary\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 11 Jun 2025   Deviance:                       2143.3\nTime:                        16:15:58   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe Poisson regression results from statsmodels.GLM show that being a Blueprinty customer is associated with a 23% increase in expected patent counts (exp(0.2076)≈1.23, and this effect is statistically significant (𝑝&lt;0.001). Age has a positive effect while age squared has a negative effect, suggesting diminishing returns to firm age. Regional effects are small and not statistically significant. Overall, the model suggests that Blueprinty customers tend to have more patents, even after accounting for firm age and region.\nWe now estimate the average increase in predicted patent counts if every firm were a Blueprinty customer versus if none were, using our fitted GLM model:\n\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_0 = X_0.astype(float)\n\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\nX_1 = X_1.astype(float)\n\n# Predict with GLM\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Average difference in predicted patents\navg_difference = np.mean(y_pred_1 - y_pred_0)\nprint(f\"Average predicted increase in patents if every firm used Blueprinty: {avg_difference:.3f}\")\n\nAverage predicted increase in patents if every firm used Blueprinty: 0.793"
  },
  {
    "objectID": "projects/hw2-mle/index.html#blueprinty-case-study",
    "href": "projects/hw2-mle/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load Blueprinty data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Histogram of number of patents by customer status\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", kde=True, bins=20)\nplt.title(\"Number of Patents by Customer Status\")\nplt.show()\n\n# Compare mean number of patents\nprint(df.groupby(\"iscustomer\")[\"patents\"].mean())\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nThe histogram shows that both customers and non-customers of Blueprinty have similar distributions centered around 2 to 4 patents. However, customers tend to have a slightly wider spread and a longer right tail, indicating that some of them have more patents than non-customers. This suggests that firms using Blueprinty’s software may be more likely to have higher numbers of patents, supporting the marketing team’s claim.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Count of regions by customer status\nprint(df.groupby(\"iscustomer\")[\"region\"].value_counts())\n\n# Compare average age by customer status\nprint(df.groupby(\"iscustomer\")[\"age\"].mean())\n\niscustomer  region   \n0           Northeast    273\n            Southwest    245\n            Midwest      187\n            Northwest    158\n            South        156\n1           Northeast    328\n            Southwest     52\n            Midwest       37\n            South         35\n            Northwest     29\nName: count, dtype: int64\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n# Histogram for age\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", bins=20, kde=True)\nplt.title(\"Firm Age by Customer Status\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Bar chart for region distribution\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Region by Customer Status\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the output, we can see that Blueprinty customers are not evenly distributed across regions. A large majority of customers come from the Northeast, while regions like the Southwest, Midwest, South, and Northwest have far fewer customers compared to non-customers. This suggests a strong regional skew in customer distribution. Additionally, the average age of customer firms (26.9 years) is slightly higher than that of non-customers (26.1 years), indicating that Blueprinty customers may be somewhat more established. These differences suggest that region and age could be confounding factors and should be accounted for in the analysis.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for ( Y () ) is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nWe now define the log-likelihood function for the Poisson model, which takes a scalar () and a vector of observed counts (Y):\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Log-likelihood for Poisson distribution.\n    Parameters:\n      lmbda: scalar lambda value\n      y: array or list of observed counts\n    Returns:\n      total log-likelihood as a float\n    \"\"\"\n\n\n    y = np.array(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\nimport matplotlib.pyplot as plt\n\n# Observed patent counts\ny_obs = df[\"patents\"].values\n\n# Range of lambda values\nlambda_vals = np.linspace(0.1, 20, 200)\nloglikelihoods = [poisson_log_likelihood(lmbda, y_obs) for lmbda in lambda_vals]\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglikelihoods)\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood of Poisson Model Across Lambda Values\")\nplt.grid(True)\nplt.show()\n\nlambda_mle_analytical = np.mean(y_obs)\nlambda_mle_analytical\n\nfrom scipy.optimize import minimize\n\n# We minimize the negative log-likelihood\nneg_log_likelihood = lambda lmbda: -poisson_log_likelihood(lmbda[0], y_obs)\n\n# Start from an initial guess, e.g., lambda = 1\nresult = minimize(neg_log_likelihood, x0=[1], bounds=[(0.01, None)])\nlambda_mle_numerical = result.x[0]\nlambda_mle_numerical\n\nprint(f\"Analytical MLE (mean of Y): {lambda_mle_analytical:.4f}\")\nprint(f\"Numerical MLE (via optimization): {lambda_mle_numerical:.4f}\")\n\n\n\n\n\n\n\n\nAnalytical MLE (mean of Y): 3.6847\nNumerical MLE (via optimization): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now define the Poisson regression log-likelihood function, where the expected number of patents (_i) is modeled as a function of firm characteristics:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_log_likelihood(beta, y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    # Compute linear predictor\n    linear_predictor = X @ beta\n\n    # Prevent overflow in exp\n    linear_predictor = np.clip(linear_predictor, -300, 50)\n\n    lambda_i = np.exp(linear_predictor)\n\n    return np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n\nWe now estimate the Poisson regression model via maximum likelihood and compare it to the built-in GLM function.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport statsmodels.api as sm\n\n# 1. Feature engineering\ndf[\"age_sq\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# 2. Build design matrix X\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_sq\"]],\n    region_dummies,\n    df[[\"iscustomer\"]]\n], axis=1)\n\ny = df[\"patents\"]\n\n# Convert to NumPy arrays with explicit dtype\nX_np = X.to_numpy(dtype=np.float64)\ny_np = y.to_numpy(dtype=np.float64)\n\n# 3. Define Poisson regression log-likelihood\ndef poisson_regression_log_likelihood(beta, y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    # Compute λ = exp(Xβ)\n    linear_predictor = X @ beta\n    lambda_i = np.exp(linear_predictor)\n\n    return np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n\n# 4. Minimize the negative log-likelihood\ndef neg_log_likelihood(beta):\n    return -poisson_regression_log_likelihood(beta, y_np, X_np)\n\ninit_beta = np.zeros(X_np.shape[1])\n\nresult = minimize(neg_log_likelihood, init_beta, method=\"BFGS\")\n\n# 5. Extract coefficients and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# 6. Output results as a table\nresults_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errors\n})\nprint(results_df)\n\n     Variable  Coefficient  Std. Error\n0   intercept     1.480059         1.0\n1         age    38.016417         1.0\n2      age_sq  1033.539585         1.0\n3   Northeast     0.640979         1.0\n4   Northwest     0.164288         1.0\n5       South     0.181562         1.0\n6   Southwest     0.295497         1.0\n7  iscustomer     0.553874         1.0\n\n\nWe now check the results using Python’s built-in statsmodels.GLM function:\n\nimport statsmodels.api as sm\n\n# Ensure all inputs are numeric\nX_clean = X.astype(float)\ny_clean = y.astype(float)\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y_clean, X_clean, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Show summary\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 11 Jun 2025   Deviance:                       2143.3\nTime:                        16:15:58   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nage_sq        -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\nThe Poisson regression results from statsmodels.GLM show that being a Blueprinty customer is associated with a 23% increase in expected patent counts (exp(0.2076)≈1.23, and this effect is statistically significant (𝑝&lt;0.001). Age has a positive effect while age squared has a negative effect, suggesting diminishing returns to firm age. Regional effects are small and not statistically significant. Overall, the model suggests that Blueprinty customers tend to have more patents, even after accounting for firm age and region.\nWe now estimate the average increase in predicted patent counts if every firm were a Blueprinty customer versus if none were, using our fitted GLM model:\n\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_0 = X_0.astype(float)\n\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\nX_1 = X_1.astype(float)\n\n# Predict with GLM\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Average difference in predicted patents\navg_difference = np.mean(y_pred_1 - y_pred_0)\nprint(f\"Average predicted increase in patents if every firm used Blueprinty: {avg_difference:.3f}\")\n\nAverage predicted increase in patents if every firm used Blueprinty: 0.793"
  },
  {
    "objectID": "projects/hw2-mle/index.html#airbnb-case-study",
    "href": "projects/hw2-mle/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Load dataset\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Preview data\ndf.head()\n\n# Drop rows with missing values in relevant fields\ncols_to_use = [\n    \"number_of_reviews\", \"price\", \"bedrooms\", \"bathrooms\", \"room_type\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\", \"days\"\n]\ndf_clean = df[cols_to_use].dropna()\n\n# Convert categorical variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_clean = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\n\n# Inspect cleaned data\ndf_clean.describe()\n\n# Distribution of number of reviews\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.show()\n\n# Correlation matrix\nsns.heatmap(df_clean.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n# Set up design matrix\ny = df_clean[\"number_of_reviews\"]\nX = df_clean.drop(columns=[\"number_of_reviews\"])\nX = sm.add_constant(X)  # add intercept\n\n# Fix data types\nX = X.astype(float)\ny = y.astype(float)\n\n# Fit Poisson model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Output summary\nprint(poisson_results.summary())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30149\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Wed, 11 Jun 2025   Deviance:                   9.2689e+05\nTime:                        16:15:58   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6840\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.4980      0.016    217.396      0.000       3.467       3.530\nprice                     -1.791e-05   8.33e-06     -2.151      0.031   -3.42e-05   -1.59e-06\nbedrooms                      0.0741      0.002     37.197      0.000       0.070       0.078\nbathrooms                    -0.1177      0.004    -31.394      0.000      -0.125      -0.110\nreview_scores_cleanliness     0.1131      0.001     75.611      0.000       0.110       0.116\nreview_scores_location       -0.0769      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0911      0.002    -50.490      0.000      -0.095      -0.088\ninstant_bookable              0.3459      0.003    119.666      0.000       0.340       0.352\ndays                       5.072e-05   3.91e-07    129.755      0.000       5e-05    5.15e-05\nroom_type_Private room       -0.0105      0.003     -3.847      0.000      -0.016      -0.005\nroom_type_Shared room        -0.2463      0.009    -28.578      0.000      -0.263      -0.229\n=============================================================================================\n\n\nThe Poisson regression results suggest that several listing characteristics are significantly associated with the number of Airbnb reviews, used here as a proxy for bookings. Listings that are instantly bookable receive about 41% more reviews than others (exp(0.346)≈1.41). Higher cleanliness scores are positively associated with more reviews, while surprisingly, higher location and value scores show small but significant negative associations. Each additional bedroom increases expected reviews by about 7.7%, whereas more bathrooms slightly reduce them. Listings with higher prices receive marginally fewer reviews. Finally, compared to entire homes, shared rooms receive substantially fewer reviews (−0.246, or 22% less), while private rooms have a negligible effect. Overall, model fit is strong with a pseudo-R2 of 0.684, indicating that the features included explain much of the variation in review counts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Namaah Hans",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]